{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM with Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import logging\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, LSTM, GRU, Dropout\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.read_csv('Data_over_sampled/dataset.csv',sep=',',names=['Msg','Tag'], header=None)\n",
    "data1 = pd.read_csv('Data_over_sampled/dataset_POS.csv',sep=',',names=['Msg','Tag'], header=None)\n",
    "data2 = pd.read_csv('Data_over_sampled/dataset_stemmed.csv',sep=',',names=['Msg','Tag'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Msg</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The New Jersey classroom half nonwhite</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sense make hispanics securing border think loy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DuetschGirlTX No I live tiny ass town one 100 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Every cultured society things therein created ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well misery I hope help 1 100 cm 1000 mm 1 254...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19003</th>\n",
       "      <td>The site transcript offers major Hitler speech...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19004</th>\n",
       "      <td>The BLMtards crashed LBBQWTF parade last month...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19005</th>\n",
       "      <td>Blacks woman pulls gun bus another woman talki...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19006</th>\n",
       "      <td>httpifamericakneworg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19007</th>\n",
       "      <td>first aint weirdo joking second inbox may empt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19008 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Msg  Tag\n",
       "0                The New Jersey classroom half nonwhite     1\n",
       "1      sense make hispanics securing border think loy...    0\n",
       "2      DuetschGirlTX No I live tiny ass town one 100 ...    1\n",
       "3      Every cultured society things therein created ...    1\n",
       "4      Well misery I hope help 1 100 cm 1000 mm 1 254...    0\n",
       "...                                                  ...  ...\n",
       "19003  The site transcript offers major Hitler speech...    0\n",
       "19004  The BLMtards crashed LBBQWTF parade last month...    1\n",
       "19005  Blacks woman pulls gun bus another woman talki...    1\n",
       "19006                               httpifamericakneworg    0\n",
       "19007  first aint weirdo joking second inbox may empt...    0\n",
       "\n",
       "[19008 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x=data[\"Msg\"]\n",
    "data_y=data[\"Tag\"]\n",
    "\n",
    "data1_x=data1[\"Msg\"]\n",
    "data1_y=data1[\"Tag\"]\n",
    "\n",
    "data2_x=data2[\"Msg\"]\n",
    "data2_y=data2[\"Tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The New Jersey classroom half nonwhite ',\n",
       "       'sense make hispanics securing border think loyalties native americans let support motivated wn good record becoming border patrol agents ',\n",
       "       'DuetschGirlTX No I live tiny ass town one 100 white folks maybe less near San Antonio many dirts around stick together comes White Girl town gotta back ALWAYS Is ',\n",
       "       ...,\n",
       "       'Blacks woman pulls gun bus another woman talking loud phone black man difuses situation ',\n",
       "       'httpifamericakneworg',\n",
       "       'first aint weirdo joking second inbox may empty check sent message list'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x_n = data_x.to_numpy()\n",
    "data_x_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RegardsNNS', 'YouTubeNN',\n",
       "       'downloadNN colorfullyRB illustratedVBD 132CD pageNN ebookNN destructionNN civilizationNN',\n",
       "       ..., 'IsVBZ hardJJ comprehendNN planNN UtopianJJ',\n",
       "       'NothingNN happenedVBD exceptIN IPRP gotVBD 88CD paperNN',\n",
       "       'HePRP tellingVBG truthRB usualJJ filthyJJ jewsNNS needVBP clampNNS soonRB possibleJJ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1_x_n = data1_x.to_numpy()\n",
    "data1_x_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['theyPRP needVBP giveJJ mandatoriNN dnaNN testNN kickNN francNN',\n",
       "       'IPRP thinkVBP asianJJ hotJJ arentNN girlNN IPRP wannaVBP getVB withaJJ wolfNN prettiNNS wannaVBP petJJ oneCD whiteJJ girlNN oneCD trustNN',\n",
       "       'WePRP handwrittenVBP accountJJ battlNN shilohVBD 4thCD tennesseNN',\n",
       "       ...,\n",
       "       'greitNN duNN erNN fornøydNN medVBD dinJJ nasjonalitetJJ detNN erNN ikkNN dètNN detNN erNN snakkNN omNN såNN slappNN avNN kameratNN',\n",
       "       'theirPRP$ lieNN thickNN maniNN peoplNN ntJJ seeVBP truthJJ frontNN',\n",
       "       'whatWP goodJJ scammerNN countriNN brokeVBD lawNN comeNN'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2_x_n = data2_x.to_numpy()\n",
    "data2_x_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sen_length(data):\n",
    "    mx_len = 0\n",
    "    for sen in data:\n",
    "        words = sen.split()\n",
    "        if len(words) > mx_len:\n",
    "            mx_len = len(words)\n",
    "    return mx_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_len = max_sen_length(data_x_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_len1 = max_sen_length(data1_x_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_len2 = max_sen_length(data2_x_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_enc = [one_hot(sen, vocab_size) for sen in data_x_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot1_enc = [one_hot(sen, vocab_size) for sen in data1_x_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot2_enc = [one_hot(sen, vocab_size) for sen in data2_x_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_repr = pad_sequences(onehot_enc, padding='pre', maxlen=sen_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_repr1 = pad_sequences(onehot1_enc, padding='pre', maxlen=sen_len1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_repr2 = pad_sequences(onehot2_enc, padding='pre', maxlen=sen_len2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0 9586 6072 4463 6375 9027 1979]\n"
     ]
    }
   ],
   "source": [
    "print(embed_repr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0 2283]\n"
     ]
    }
   ],
   "source": [
    "print(embed_repr1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0 7869 3861 9928 6979 3366\n",
      " 7748 7876 2008]\n"
     ]
    }
   ],
   "source": [
    "print(embed_repr2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LSTM Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_features = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 151, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,080,501\n",
      "Trainable params: 1,080,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_features,input_length=sen_len))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 144, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,080,501\n",
      "Trainable params: 1,080,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(vocab_size, embedding_vector_features,input_length=sen_len1))\n",
    "model1.add(LSTM(100))\n",
    "model1.add(Dense(1,activation='sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 143, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,080,501\n",
      "Trainable params: 1,080,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size, embedding_vector_features,input_length=sen_len2))\n",
    "model2.add(LSTM(100))\n",
    "model2.add(Dense(1,activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(embed_repr, data_y, test_size=0.2, random_state=4)\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(embed_repr1, data1_y, test_size=0.2, random_state=4)\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(embed_repr2, data2_y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15206 samples, validate on 3802 samples\n",
      "Epoch 1/10\n",
      "15206/15206 [==============================] - 42s 3ms/sample - loss: 0.4501 - acc: 0.7773 - val_loss: 0.2715 - val_acc: 0.8948\n",
      "Epoch 2/10\n",
      "15206/15206 [==============================] - 43s 3ms/sample - loss: 0.1636 - acc: 0.9446 - val_loss: 0.1840 - val_acc: 0.9358\n",
      "Epoch 3/10\n",
      "15206/15206 [==============================] - 44s 3ms/sample - loss: 0.0760 - acc: 0.9771 - val_loss: 0.2223 - val_acc: 0.9316\n",
      "Epoch 4/10\n",
      "15206/15206 [==============================] - 43s 3ms/sample - loss: 0.0461 - acc: 0.9876 - val_loss: 0.1590 - val_acc: 0.9569\n",
      "Epoch 5/10\n",
      "15206/15206 [==============================] - 43s 3ms/sample - loss: 0.0241 - acc: 0.9937 - val_loss: 0.1783 - val_acc: 0.9558\n",
      "Epoch 6/10\n",
      "15206/15206 [==============================] - 43s 3ms/sample - loss: 0.0142 - acc: 0.9961 - val_loss: 0.1872 - val_acc: 0.9584\n",
      "Epoch 7/10\n",
      "15206/15206 [==============================] - 43s 3ms/sample - loss: 0.0135 - acc: 0.9963 - val_loss: 0.1946 - val_acc: 0.9566\n",
      "Epoch 8/10\n",
      "15206/15206 [==============================] - 47s 3ms/sample - loss: 0.0121 - acc: 0.9962 - val_loss: 0.2901 - val_acc: 0.9371\n",
      "Epoch 9/10\n",
      "15206/15206 [==============================] - 46s 3ms/sample - loss: 0.0101 - acc: 0.9973 - val_loss: 0.2434 - val_acc: 0.9540\n",
      "Epoch 10/10\n",
      "15206/15206 [==============================] - 50s 3ms/sample - loss: 0.0056 - acc: 0.9986 - val_loss: 0.2582 - val_acc: 0.9590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb12a1a67b8>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train, validation_data=(x_test,y_test),epochs=10,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14940 samples, validate on 3736 samples\n",
      "Epoch 1/10\n",
      "14940/14940 [==============================] - 47s 3ms/sample - loss: 0.4560 - acc: 0.7753 - val_loss: 0.2872 - val_acc: 0.8857\n",
      "Epoch 2/10\n",
      "14940/14940 [==============================] - 46s 3ms/sample - loss: 0.1489 - acc: 0.9491 - val_loss: 0.1753 - val_acc: 0.9400\n",
      "Epoch 3/10\n",
      "14940/14940 [==============================] - 46s 3ms/sample - loss: 0.0668 - acc: 0.9781 - val_loss: 0.1591 - val_acc: 0.9526\n",
      "Epoch 4/10\n",
      "14940/14940 [==============================] - 46s 3ms/sample - loss: 0.0233 - acc: 0.9926 - val_loss: 0.2159 - val_acc: 0.9475\n",
      "Epoch 5/10\n",
      "14940/14940 [==============================] - 46s 3ms/sample - loss: 0.0128 - acc: 0.9957 - val_loss: 0.1806 - val_acc: 0.9639\n",
      "Epoch 6/10\n",
      "14940/14940 [==============================] - 46s 3ms/sample - loss: 0.0083 - acc: 0.9972 - val_loss: 0.2479 - val_acc: 0.9489\n",
      "Epoch 7/10\n",
      "14940/14940 [==============================] - 46s 3ms/sample - loss: 0.0082 - acc: 0.9979 - val_loss: 0.2036 - val_acc: 0.9604\n",
      "Epoch 8/10\n",
      "14940/14940 [==============================] - 45s 3ms/sample - loss: 0.0101 - acc: 0.9969 - val_loss: 0.2202 - val_acc: 0.9593\n",
      "Epoch 9/10\n",
      "14940/14940 [==============================] - 46s 3ms/sample - loss: 0.0101 - acc: 0.9965 - val_loss: 0.2258 - val_acc: 0.9612\n",
      "Epoch 10/10\n",
      "14940/14940 [==============================] - 46s 3ms/sample - loss: 0.0058 - acc: 0.9982 - val_loss: 0.1976 - val_acc: 0.9673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb129837a20>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(x1_train,y1_train, validation_data=(x1_test,y1_test),epochs=10,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15195 samples, validate on 3799 samples\n",
      "Epoch 1/10\n",
      "15195/15195 [==============================] - 47s 3ms/sample - loss: 0.4314 - acc: 0.7940 - val_loss: 0.2722 - val_acc: 0.8931\n",
      "Epoch 2/10\n",
      "15195/15195 [==============================] - 46s 3ms/sample - loss: 0.1498 - acc: 0.9491 - val_loss: 0.1885 - val_acc: 0.9366\n",
      "Epoch 3/10\n",
      "15195/15195 [==============================] - 45s 3ms/sample - loss: 0.0764 - acc: 0.9764 - val_loss: 0.2181 - val_acc: 0.9360\n",
      "Epoch 4/10\n",
      "15195/15195 [==============================] - 43s 3ms/sample - loss: 0.0346 - acc: 0.9906 - val_loss: 0.1785 - val_acc: 0.9537\n",
      "Epoch 5/10\n",
      "15195/15195 [==============================] - 45s 3ms/sample - loss: 0.0233 - acc: 0.9938 - val_loss: 0.1997 - val_acc: 0.9458\n",
      "Epoch 6/10\n",
      "15195/15195 [==============================] - 44s 3ms/sample - loss: 0.0132 - acc: 0.9967 - val_loss: 0.2798 - val_acc: 0.9387\n",
      "Epoch 7/10\n",
      "15195/15195 [==============================] - 43s 3ms/sample - loss: 0.0088 - acc: 0.9976 - val_loss: 0.1935 - val_acc: 0.9595\n",
      "Epoch 8/10\n",
      "15195/15195 [==============================] - 45s 3ms/sample - loss: 0.0047 - acc: 0.9985 - val_loss: 0.3148 - val_acc: 0.9408\n",
      "Epoch 9/10\n",
      "15195/15195 [==============================] - 45s 3ms/sample - loss: 0.0040 - acc: 0.9989 - val_loss: 0.3288 - val_acc: 0.9463\n",
      "Epoch 10/10\n",
      "15195/15195 [==============================] - 46s 3ms/sample - loss: 0.0061 - acc: 0.9982 - val_loss: 0.3819 - val_acc: 0.9321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb128f2a668>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x2_train,y2_train, validation_data=(x2_test,y2_test),epochs=10,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and Model Accuracy (Without POS and Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1729,  148],\n",
       "       [   8, 1917]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9589689637033141"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9609022556390978"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9283292978208233"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9958441558441559"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and Model Accuracy (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_pred = model1.predict_classes(x1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1730,  122],\n",
       "       [   0, 1884]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y1_test, y1_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9673447537473233"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y1_test, y1_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9686375321336761"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y1_test, y1_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9391824526420738"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y1_test, y1_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y1_test, y1_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and Model Accuracy (Stemming + POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_pred = model2.predict_classes(x2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1635,  246],\n",
       "       [  12, 1906]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y2_test, y2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9320873914187944"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y2_test, y2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9366093366093367"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y2_test, y2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8856877323420075"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y2_test, y2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9958441558441559"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y2_test, y2_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.read_csv('Data_under_sampled/dataset.csv',sep=',',names=['Msg','Tag'])\n",
    "data1 = pd.read_csv('Data_under_sampled/dataset_POS.csv',sep=',',names=['Msg','Tag'])\n",
    "data2 = pd.read_csv('Data_under_sampled/dataset_stemmed.csv',sep=',',names=['Msg','Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x=data[\"Msg\"]\n",
    "data_y=data[\"Tag\"]\n",
    "\n",
    "data1_x=data1[\"Msg\"]\n",
    "data1_y=data1[\"Tag\"]\n",
    "\n",
    "data2_x=data2[\"Msg\"]\n",
    "data2_y=data2[\"Tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "data_x_n = data_x.to_numpy()\n",
    "data1_x_n = data1_x.to_numpy()\n",
    "data2_x_n = data2_x.to_numpy()\n",
    "sen_len = max_sen_length(data_x_n)\n",
    "sen_len1 = max_sen_length(data1_x_n)\n",
    "sen_len2 = max_sen_length(data2_x_n)\n",
    "onehot_enc = [one_hot(sen, vocab_size) for sen in data_x_n]\n",
    "onehot1_enc = [one_hot(sen, vocab_size) for sen in data1_x_n]\n",
    "onehot2_enc = [one_hot(sen, vocab_size) for sen in data2_x_n]\n",
    "embed_repr = pad_sequences(onehot_enc, padding='pre', maxlen=sen_len)\n",
    "embed_repr1 = pad_sequences(onehot1_enc, padding='pre', maxlen=sen_len1)\n",
    "embed_repr2 = pad_sequences(onehot2_enc, padding='pre', maxlen=sen_len2)\n",
    "embedding_vector_features = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(embed_repr, data_y, test_size=0.2, random_state=4)\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(embed_repr1, data1_y, test_size=0.2, random_state=4)\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(embed_repr2, data2_y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without POS + Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 144, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,080,501\n",
      "Trainable params: 1,080,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_features,input_length=sen_len))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2294 samples, validate on 574 samples\n",
      "Epoch 1/10\n",
      "2294/2294 [==============================] - 8s 4ms/sample - loss: 0.6628 - acc: 0.6024 - val_loss: 0.6706 - val_acc: 0.5993\n",
      "Epoch 2/10\n",
      "2294/2294 [==============================] - 7s 3ms/sample - loss: 0.5165 - acc: 0.7428 - val_loss: 0.6365 - val_acc: 0.6742\n",
      "Epoch 3/10\n",
      "2294/2294 [==============================] - 7s 3ms/sample - loss: 0.2655 - acc: 0.8976 - val_loss: 0.7923 - val_acc: 0.6707\n",
      "Epoch 4/10\n",
      "2294/2294 [==============================] - 7s 3ms/sample - loss: 0.1252 - acc: 0.9555 - val_loss: 1.0100 - val_acc: 0.6568\n",
      "Epoch 5/10\n",
      "2294/2294 [==============================] - 7s 3ms/sample - loss: 0.1559 - acc: 0.9538 - val_loss: 0.8361 - val_acc: 0.6498\n",
      "Epoch 6/10\n",
      "2294/2294 [==============================] - 7s 3ms/sample - loss: 0.0628 - acc: 0.9843 - val_loss: 1.0824 - val_acc: 0.6620\n",
      "Epoch 7/10\n",
      "2294/2294 [==============================] - 7s 3ms/sample - loss: 0.0324 - acc: 0.9922 - val_loss: 1.1583 - val_acc: 0.6638\n",
      "Epoch 8/10\n",
      "2294/2294 [==============================] - 7s 3ms/sample - loss: 0.0231 - acc: 0.9956 - val_loss: 1.2946 - val_acc: 0.6551\n",
      "Epoch 9/10\n",
      "2294/2294 [==============================] - 7s 3ms/sample - loss: 0.0118 - acc: 0.9983 - val_loss: 1.4112 - val_acc: 0.6516\n",
      "Epoch 10/10\n",
      "2294/2294 [==============================] - 7s 3ms/sample - loss: 0.0083 - acc: 0.9987 - val_loss: 1.4931 - val_acc: 0.6533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb12c2ad9e8>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train, validation_data=(x_test,y_test),epochs=10,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[191  94]\n",
      " [105 184]]\n",
      "0.6533101045296167\n",
      "0.6490299823633158\n",
      "0.6618705035971223\n",
      "0.6366782006920415\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(x_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred))\n",
    "print(precision_score(y_test, y_pred))\n",
    "print(recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 144, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,080,501\n",
      "Trainable params: 1,080,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(vocab_size, embedding_vector_features,input_length=sen_len1))\n",
    "model1.add(LSTM(100))\n",
    "model1.add(Dense(1,activation='sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2281 samples, validate on 571 samples\n",
      "Epoch 1/10\n",
      "2281/2281 [==============================] - 8s 3ms/sample - loss: 0.6758 - acc: 0.5809 - val_loss: 0.6613 - val_acc: 0.5954\n",
      "Epoch 2/10\n",
      "2281/2281 [==============================] - 7s 3ms/sample - loss: 0.5482 - acc: 0.7256 - val_loss: 0.6328 - val_acc: 0.6550\n",
      "Epoch 3/10\n",
      "2281/2281 [==============================] - 7s 3ms/sample - loss: 0.2884 - acc: 0.8803 - val_loss: 0.8140 - val_acc: 0.6637\n",
      "Epoch 4/10\n",
      "2281/2281 [==============================] - 7s 3ms/sample - loss: 0.1259 - acc: 0.9605 - val_loss: 1.2058 - val_acc: 0.6462\n",
      "Epoch 5/10\n",
      "2281/2281 [==============================] - 7s 3ms/sample - loss: 0.0748 - acc: 0.9860 - val_loss: 1.1977 - val_acc: 0.6532\n",
      "Epoch 6/10\n",
      "2281/2281 [==============================] - 7s 3ms/sample - loss: 0.0335 - acc: 0.9925 - val_loss: 1.5501 - val_acc: 0.6375\n",
      "Epoch 7/10\n",
      "2281/2281 [==============================] - 7s 3ms/sample - loss: 0.0232 - acc: 0.9947 - val_loss: 1.4084 - val_acc: 0.6340\n",
      "Epoch 8/10\n",
      "2281/2281 [==============================] - 7s 3ms/sample - loss: 0.0130 - acc: 0.9987 - val_loss: 1.8212 - val_acc: 0.6532\n",
      "Epoch 9/10\n",
      "2281/2281 [==============================] - 6s 3ms/sample - loss: 0.0080 - acc: 0.9991 - val_loss: 2.0473 - val_acc: 0.6322\n",
      "Epoch 10/10\n",
      "2281/2281 [==============================] - 6s 3ms/sample - loss: 0.0076 - acc: 0.9978 - val_loss: 1.8001 - val_acc: 0.6392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb122a80e80>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(x1_train,y1_train, validation_data=(x1_test,y1_test),epochs=10,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[181 101]\n",
      " [105 184]]\n",
      "0.6392294220665499\n",
      "0.6411149825783972\n",
      "0.6456140350877193\n",
      "0.6366782006920415\n"
     ]
    }
   ],
   "source": [
    "y1_pred = model1.predict_classes(x1_test)\n",
    "print(confusion_matrix(y1_test, y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))\n",
    "print(f1_score(y1_test, y1_pred))\n",
    "print(precision_score(y1_test, y1_pred))\n",
    "print(recall_score(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS + Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 143, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,080,501\n",
      "Trainable params: 1,080,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size, embedding_vector_features,input_length=sen_len2))\n",
    "model2.add(LSTM(100))\n",
    "model2.add(Dense(1,activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2294 samples, validate on 574 samples\n",
      "Epoch 1/10\n",
      "2294/2294 [==============================] - 7s 3ms/sample - loss: 0.6696 - acc: 0.5990 - val_loss: 0.6493 - val_acc: 0.6272\n",
      "Epoch 2/10\n",
      "2294/2294 [==============================] - 6s 3ms/sample - loss: 0.5261 - acc: 0.7367 - val_loss: 0.6098 - val_acc: 0.6829\n",
      "Epoch 3/10\n",
      "2294/2294 [==============================] - 6s 3ms/sample - loss: 0.2692 - acc: 0.8997 - val_loss: 0.7204 - val_acc: 0.6707\n",
      "Epoch 4/10\n",
      "2294/2294 [==============================] - 6s 3ms/sample - loss: 0.1073 - acc: 0.9634 - val_loss: 0.9734 - val_acc: 0.6620\n",
      "Epoch 5/10\n",
      "2294/2294 [==============================] - 7s 3ms/sample - loss: 0.0528 - acc: 0.9847 - val_loss: 1.1859 - val_acc: 0.6551\n",
      "Epoch 6/10\n",
      "2294/2294 [==============================] - 7s 3ms/sample - loss: 0.0394 - acc: 0.9891 - val_loss: 0.9721 - val_acc: 0.6655\n",
      "Epoch 7/10\n",
      "2294/2294 [==============================] - 7s 3ms/sample - loss: 0.0358 - acc: 0.9935 - val_loss: 1.0932 - val_acc: 0.6603\n",
      "Epoch 8/10\n",
      "2294/2294 [==============================] - 7s 3ms/sample - loss: 0.0167 - acc: 0.9969 - val_loss: 1.3712 - val_acc: 0.6760\n",
      "Epoch 9/10\n",
      "2294/2294 [==============================] - 7s 3ms/sample - loss: 0.0126 - acc: 0.9965 - val_loss: 1.4355 - val_acc: 0.6725\n",
      "Epoch 10/10\n",
      "2294/2294 [==============================] - 7s 3ms/sample - loss: 0.0082 - acc: 0.9987 - val_loss: 1.4851 - val_acc: 0.6516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb100a4d898>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x2_train,y2_train, validation_data=(x2_test,y2_test),epochs=10,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[168 121]\n",
      " [ 79 206]]\n",
      "0.6515679442508711\n",
      "0.6732026143790849\n",
      "0.6299694189602446\n",
      "0.7228070175438597\n"
     ]
    }
   ],
   "source": [
    "y2_pred = model2.predict_classes(x2_test)\n",
    "print(confusion_matrix(y2_test, y2_pred))\n",
    "print(accuracy_score(y2_test, y2_pred))\n",
    "print(f1_score(y2_test, y2_pred))\n",
    "print(precision_score(y2_test, y2_pred))\n",
    "print(recall_score(y2_test, y2_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
