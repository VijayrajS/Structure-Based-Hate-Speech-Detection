{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "N_Gram2.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI5J4dpLT2bG"
      },
      "source": [
        "# N Gram model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjJiyroeT2bS"
      },
      "source": [
        "### Importing prerequisite libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzKFsqNLT2bV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dacfdba-e154-4bf3-cacb-87b0c2ea8353"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGeX-Rs_T2bq"
      },
      "source": [
        "### Loading datasets and dropping nulls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0REa9i2BT2bs"
      },
      "source": [
        "data = pd.read_csv('data_raw.csv',sep=',',names=['Msg','Tag'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UTtOLhOT2cF"
      },
      "source": [
        "data.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mytoY4npT2c8"
      },
      "source": [
        "data_x=data[\"Msg\"]\n",
        "data_y=data[\"Tag\"]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=4)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLJvyNImCis7"
      },
      "source": [
        "en_stopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "def tokenize(text):\n",
        "  return text.split()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP3po75bRFdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b25df75b-879f-4f53-873f-a3dc858e6c87"
      },
      "source": [
        "types = [(1,1),(1,2),(1,3),(1,4)]\n",
        "names = ['unigram','bigram','trigram','four gram']\n",
        "ans = {}\n",
        "ans['model'] = []\n",
        "ans['F1-score'] = []\n",
        "ans['Recall'] = []\n",
        "ans['Accuracy'] = []\n",
        "ans['Precision'] = []\n",
        "\n",
        "for i in range(len(types)):\n",
        "    vectorizer = CountVectorizer(\n",
        "        analyzer = 'word',\n",
        "        lowercase = True,\n",
        "        tokenizer = tokenize,\n",
        "        ngram_range=types[i],\n",
        "        stop_words = en_stopwords)\n",
        "\n",
        "    vectorizer.fit(x_train)\n",
        "    train_set = vectorizer.transform(x_train)\n",
        "    test_set = vectorizer.transform(x_test)\n",
        "    string = names[i]+' '+'using LR'\n",
        "\n",
        "    #Testing results with LR\n",
        "    lr = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial')  \n",
        "    lr.fit(train_set, y_train)\n",
        "    prediction = lr.predict(test_set)\n",
        "    \n",
        "    f1 = f1_score(y_test, prediction, average='weighted')\n",
        "    acc = accuracy_score(y_test, prediction)\n",
        "    rec = recall_score(y_test, prediction, average = 'macro')\n",
        "    pre = precision_score(y_test, prediction, average='macro')\n",
        "    \n",
        "    ans['model'].append(string)\n",
        "    ans['F1-score'].append(f1)\n",
        "    ans['Recall'].append(rec)\n",
        "    ans['Accuracy'].append(acc)\n",
        "    ans['Precision'].append(pre)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeguIrg5EQJT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "032fb9b6-bd71-400c-c9ee-ded6257a3570"
      },
      "source": [
        "import operator\n",
        "final = pd.DataFrame(ans)\n",
        "final"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>F1-score</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>unigram using LR</td>\n",
              "      <td>0.896763</td>\n",
              "      <td>0.688481</td>\n",
              "      <td>0.905588</td>\n",
              "      <td>0.739073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bigram using LR</td>\n",
              "      <td>0.898892</td>\n",
              "      <td>0.690022</td>\n",
              "      <td>0.909018</td>\n",
              "      <td>0.765542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>trigram using LR</td>\n",
              "      <td>0.895570</td>\n",
              "      <td>0.678172</td>\n",
              "      <td>0.907202</td>\n",
              "      <td>0.755497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>four gram using LR</td>\n",
              "      <td>0.893660</td>\n",
              "      <td>0.671162</td>\n",
              "      <td>0.906395</td>\n",
              "      <td>0.748288</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                model  F1-score    Recall  Accuracy  Precision\n",
              "0    unigram using LR  0.896763  0.688481  0.905588   0.739073\n",
              "1     bigram using LR  0.898892  0.690022  0.909018   0.765542\n",
              "2    trigram using LR  0.895570  0.678172  0.907202   0.755497\n",
              "3  four gram using LR  0.893660  0.671162  0.906395   0.748288"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    }
  ]
}